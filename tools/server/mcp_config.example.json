{
  "_comment": "Example MCP configuration for llama.cpp",
  "_comment_windows": "On Windows, place this file in %APPDATA%\\llama.cpp\\mcp.json",
  "_comment_macos": "On macOS/Linux, place this file in ~/.llama.cpp/mcp.json",
  "_comment_env": "Or set the LLAMA_MCP_CONFIG environment variable to point to this file",
  "mcpServers": {
    "_comment": "Remote MCP servers (proxied via C++ backend with CORS support)",
    "brave-search": {
      "_comment": "Run: BRAVE_API_KEY=... npx -y @anthropic-ai/mcp-server-brave-search --transport http --port 38180",
      "_comment_key": "Get your API key at https://brave.com/search/api/",
      "url": "http://127.0.0.1:38180/mcp"
    },
    "python": {
      "_comment": "Run: uvx mcp-run-python --deps numpy,pandas,pydantic,requests,httpx,sympy,aiohttp streamable-http --port 38181",
      "url": "http://127.0.0.1:38181/mcp"
    },
    "filesystem": {
      "_comment": "Run: npx -y @anthropic-ai/mcp-server-filesystem --transport http --port 38182 /path/to/allowed/dir",
      "url": "http://127.0.0.1:38182/mcp"
    }
  }
}
